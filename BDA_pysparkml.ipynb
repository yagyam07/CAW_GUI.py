{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagyam07/CAW_GUI.py/blob/main/BDA_pysparkml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLTt5yrfYbGN",
        "outputId": "504b40b4-9352-46f9-eefb-525d26336b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,200 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,032 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,372 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,400 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,451 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,598 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,648 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.9 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,278 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
            "Fetched 26.6 MB in 3s (8,155 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        " #Create a PySpark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "spark = SparkSession.builder.appName(\"nik\").getOrCreate()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collect_rdd = sc.parallelize([1,2,3,4,5])\n",
        "print(collect_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tByovVPmYbla",
        "outputId": "00a5013a-d73f-432d-c303-bd68f08267c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
        "print(count_rdd.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfOSgf-MYbog",
        "outputId": "55350840-2d23-4326-8652-78e9cc91c214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "print(first_rdd.first())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MP3Sbb0YbrZ",
        "outputId": "0266f0e5-4167-4a15-e2f1-9038dcd74b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "take_rdd = sc.parallelize([1,2,3,4,5])\n",
        "print(take_rdd.take(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qIHXV9FamqO",
        "outputId": "9d3b905f-323c-4a50-d612-e721118cfbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_rdd = sc.parallelize([1,3,4,6])\n",
        "print(reduce_rdd.reduce(lambda x, y : x + y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB_EX05tamtc",
        "outputId": "683c0640-2744-4467-8d65-b861b3d824db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_rdd = sc.parallelize([1,2,3,4])\n",
        "print(my_rdd.map(lambda x: x+ 10).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC_SE3Sgam0G",
        "outputId": "ec1a813e-f812-4d8e-83dd-ee069313a60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 12, 13, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
        "print(filter_rdd.filter(lambda x: x%2 == 0).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG0HxYEaam5M",
        "outputId": "2f3d1b21-c437-46fd-da02-ea35570ff489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_rdd_2 = sc.parallelize(['Rahul', 'Swati', 'Rohan', 'Shreya', 'Priya'])\n",
        "print(filter_rdd_2.filter(lambda x: x.startswith('R')).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbJfFPeOam88",
        "outputId": "383c14b6-268f-43d3-ecd3-b29a4d408a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Rahul', 'Rohan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "union_inp = sc.parallelize([2,4,5,6,7,8,9])\n",
        "union_rdd_1 = union_inp.filter(lambda x: x % 2 == 0)\n",
        "union_rdd_2 = union_inp.filter(lambda x: x % 3 == 0)\n",
        "print(union_rdd_1.union(union_rdd_2).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMl5zmfLam_0",
        "outputId": "67cd38bb-e9bb-4724-a42d-b8600a378e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 6, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
        "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGHUJ9svb3a2",
        "outputId": "a93b6c1e-bf61-493d-fee4-0feb694684e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78),('Rahul', 88), ('Swati', 92)]\n",
        "marks_rdd=sc.parallelize(marks)\n",
        "marks_rdd.collect()\n",
        "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADqkrEVfb3d2",
        "outputId": "37337338-25c7-46d5-e483-74cb0575f5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rahul', 176), ('Shreya', 83), ('Rohan', 78), ('Swati', 184), ('Abhay', 93)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyn6yHWWc6z8",
        "outputId": "2a70fad8-9478-42b9-eb7b-b30d59f9ee39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rahul', 48), ('Shreya', 50), ('Rohan', 44), ('Swati', 45), ('Abhay', 55)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "print(marks_rdd.sortByKey('ascending').collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrICq_W0dSSe",
        "outputId": "968b3473-045c-4044-aaec-a15bbfd9680c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "dict_rdd = marks_rdd.groupByKey().collect()\n",
        "for key, value in dict_rdd:\n",
        "    print(key, list(value))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O3s1LzrdSVk",
        "outputId": "60f5f189-38f0-472a-be09-64779befbe88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rahul [25, 23]\n",
            "Shreya [22, 28]\n",
            "Rohan [22, 22]\n",
            "Swati [26, 19]\n",
            "Abhay [29, 26]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "dict_rdd = marks_rdd.countByKey().items()\n",
        "for key, value in dict_rdd:\n",
        "    print(key, value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq0oR6XFdSYd",
        "outputId": "15dfc13a-c98a-408a-9abe-89ad46e1f3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rahul 2\n",
            "Swati 2\n",
            "Rohan 2\n",
            "Shreya 1\n",
            "Abhay 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r1=sc.textFile('/content/test.txt')"
      ],
      "metadata": {
        "id": "Y6STtjHcfajO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "Hlbvydmnf7yq",
        "outputId": "2165c492-0ba5-4df3-a7b4-10504ae5930a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/test.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-64b501b99cda>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/test.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJNS__NRiIjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2=sc.textFile('/content/text.txt')"
      ],
      "metadata": {
        "id": "TKHVjcs9f71g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2.collect()"
      ],
      "metadata": {
        "id": "lr6NatRyf735"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=r1.flatMap(lambda x: x.split(\" \"))"
      ],
      "metadata": {
        "id": "Wv-KhB1Wgs6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcount=words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)"
      ],
      "metadata": {
        "id": "BUivSH7Pgs9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcount.collect()"
      ],
      "metadata": {
        "id": "brbSDBhWhYjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Flights Project').getOrCreate()\n",
        "spark\n"
      ],
      "metadata": {
        "id": "L9oZf5FshYmX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "48cb40cd-7647-4e4d-99f8-1024cfeb1c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c7d67270af0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ac897d95fb78:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "UHUNkccfnAq9",
        "outputId": "cf9dc9c5-08dc-4f55-d72f-ab5a09868457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c7d67270af0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ac897d95fb78:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from CSV file\n",
        "flights = spark.read.csv('/content/flights-larger.csv', sep=',', header=True, inferSchema=True,nullValue='NA')\n",
        "flights.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keZMQRN8nAuD",
        "outputId": "f177d179-add3-402d-999a-db8fed91dd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
            "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
            "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
            "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
            "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
            "|  3| 28|  1|     B6|   377|LGA|1076| 13.33|     182|   70|\n",
            "|  5| 28|  6|     B6|   904|ORD| 740|  9.58|     130|   47|\n",
            "|  1| 19|  2|     UA|   820|SFO| 679| 12.75|     123|  135|\n",
            "|  8|  5|  5|     US|  2175|LGA| 214|  13.0|      71|  -10|\n",
            "|  5| 27|  5|     AA|  1240|ORD|1197| 14.42|     195|  -11|\n",
            "|  8| 20|  6|     B6|   119|JFK|1182| 14.67|     198|   20|\n",
            "|  2|  3|  1|     AA|  1881|JFK|1090| 15.92|     200|   -9|\n",
            "|  8| 26|  5|     B6|    35|JFK|1028| 20.58|     193|  102|\n",
            "|  4|  9|  5|     AA|   336|ORD| 733|  20.5|     125|   32|\n",
            "|  3|  8|  2|     UA|   678|ORD| 733| 10.95|     129|   55|\n",
            "|  8| 10|  3|     OH|  6347|LGA| 292| 11.75|     102|    8|\n",
            "|  8| 14|  0|     UA|   624|ORD| 612| 17.92|     109|   57|\n",
            "|  4|  8|  4|     OH|  5585|JFK| 301| 13.25|      88|   23|\n",
            "|  1| 14|  4|     UA|  1524|SFO| 414| 14.87|      91|   27|\n",
            "|  1|  2|  6|     AA|  1341|ORD|1846|   7.5|     275|   26|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9V6p1WunAxE",
        "outputId": "0e29e897-8328-4ea3-a605-ae23264ee03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data contain 275000 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column data types\n",
        "print(flights.printSchema())\n",
        "print(flights.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2H_Uoy5nA7s",
        "outputId": "c00eae2a-c416-479a-b5da-7df7cecf2c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mon: integer (nullable = true)\n",
            " |-- dom: integer (nullable = true)\n",
            " |-- dow: integer (nullable = true)\n",
            " |-- carrier: string (nullable = true)\n",
            " |-- flight: integer (nullable = true)\n",
            " |-- org: string (nullable = true)\n",
            " |-- mile: integer (nullable = true)\n",
            " |-- depart: double (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- delay: integer (nullable = true)\n",
            "\n",
            "None\n",
            "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights_drop_column = flights.drop('flight')\n",
        "# Number of records with missing 'delay' values\n",
        "flights_drop_column.filter('delay IS NULL').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foGsYJyxnA_D",
        "outputId": "85df860e-bbf4-4b21-ebea-e4d4e3fe4d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16711"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove records with missing values in any column and get the number of remaining rows\n",
        "flights_none_missing = flights_valid_delay.dropna()\n",
        "print(flights_none_missing.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAsxc-FDpwwd",
        "outputId": "cc02c40b-5299-4f11-f73f-23607a5545ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OWEo90-pwy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove records with missing 'delay' values\n",
        "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')"
      ],
      "metadata": {
        "id": "SOKJuHnCnBEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights_none_missing.withColumn('km', round(flights_none_missing.mile * 1.60934, 0)).drop('mile')\n",
        "# Create 'label' column indicating whether flight delayed (1) or not(0)\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahc-Q12YnBIt",
        "outputId": "5a3997d6-3e81-4a4d-980f-f7b36933e41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|\n",
            "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|\n",
            "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|\n",
            "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|\n",
            "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights_km)\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights_km)\n"
      ],
      "metadata": {
        "id": "zIrVEtbRnBLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  flights_indexed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxShXcEOnBN0",
        "outputId": "ef09eb5f-3202-4144-8fe2-83bef7d8bc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
            "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
            "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|\n",
            "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|\n",
            "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|\n",
            "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|\n",
            "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
      ],
      "metadata": {
        "id": "q5hX2LnGnBQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights_indexed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rukSe3W_rKcc",
        "outputId": "91b44c0a-820c-428a-81c8-611af91fcb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
            "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
            "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|    0.0|\n",
            "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|    0.0|\n",
            "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|    2.0|\n",
            "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|    5.0|\n",
            "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|    3.0|\n",
            "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=[\n",
        " 'mon', 'dom', 'dow',\n",
        " 'carrier_idx',\n",
        " 'org_idx',\n",
        " 'km', 'depart', 'duration'\n",
        "], outputCol='features')"
      ],
      "metadata": {
        "id": "5ht8GlsLrKfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights_indexed)\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features', 'delay').show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i-CVwmJreR_",
        "outputId": "2f1f3b30-35c7-4541-c6f6-3c4524acd232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------+-----+\n",
            "|features                                 |delay|\n",
            "+-----------------------------------------+-----+\n",
            "|[10.0,10.0,1.0,2.0,0.0,253.0,8.18,51.0]  |27   |\n",
            "|[11.0,22.0,1.0,2.0,0.0,1188.0,7.17,127.0]|-19  |\n",
            "|[2.0,14.0,5.0,4.0,2.0,3618.0,21.17,365.0]|60   |\n",
            "|[5.0,25.0,3.0,3.0,5.0,621.0,12.92,85.0]  |22   |\n",
            "|[3.0,28.0,1.0,4.0,3.0,1732.0,13.33,182.0]|70   |\n",
            "+-----------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights_assembled.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ro2FuceredX",
        "outputId": "cdeda210-b294-453b-8a5b-60b423beb3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258289"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test split\n",
        "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights_assembled.count()\n",
        "print(training_ratio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coaepL8HrKiY",
        "outputId": "489c6ca8-1f9e-4b9d-daff-f6373821863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7998404887548444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Decision Tree\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "# Create a classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)"
      ],
      "metadata": {
        "id": "XMtovzZ_rKoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictions for the testing data and take a look at the predictions\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4t8ANDasNwR",
        "outputId": "2b9d0abf-c376-4644-d87e-da7db09c0423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+----------------------------------------+\n",
            "|label|prediction|probability                             |\n",
            "+-----+----------+----------------------------------------+\n",
            "|1    |0.0       |[0.6225157922062938,0.3774842077937062] |\n",
            "|1    |1.0       |[0.31568636281751034,0.6843136371824896]|\n",
            "|1    |0.0       |[0.6225157922062938,0.3774842077937062] |\n",
            "|1    |1.0       |[0.31568636281751034,0.6843136371824896]|\n",
            "|0    |1.0       |[0.31568636281751034,0.6843136371824896]|\n",
            "+-----+----------+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Decision Tree\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaDbN1C8sNzJ",
        "outputId": "0a621f63-872e-4624-b5c7-40bcfa89d010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 9833|\n",
            "|    0|       0.0|16711|\n",
            "|    1|       1.0|16033|\n",
            "|    0|       1.0| 9122|\n",
            "+-----+----------+-----+\n",
            "\n",
            "0.6333584788874059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = TP / (TP + FP)\n",
        "print(precision)\n",
        "recall = TP / (TP + FN)\n",
        "print(recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6mZbSIMwTqv",
        "outputId": "93eaa8b6-22c2-41ea-cf38-1b50102dd83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6373683164380839\n",
            "0.6198484497023119\n"
          ]
        }
      ]
    }
  ]
}